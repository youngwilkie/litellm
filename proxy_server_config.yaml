litellm_settings:
  # set_verbose: True  # Uncomment this if you want to see verbose logs; not recommended in production
  drop_params: True
  enable_json_schema_validation: False
  num_retries: 3  # Retry failed requests on other deployments (CRITICAL for throughput)
  request_timeout: 600
  telemetry: False

general_settings: 
  master_key: os.environ/LITELLM_MASTER_KEY # CRITICAL: Use environment variable
  database_url: os.environ/DATABASE_URL # CRITICAL: Database connection
  database_connection_pool_limit: 100 # Database connection pool limit
  store_model_in_db: True
  store_prompts_in_spend_logs: True
  max_parallel_requests: 2000  # Handle high concurrency - adjust based on server capacity
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 10

router_settings:
  routing_strategy: simple-shuffle  # Works without Redis - uses weights if set in model_list
  num_retries: 3  # Router-level retries
  timeout: 600
  retry_after: 0  # No delay between retries = faster failover
  allowed_fails: 3  # Deployments fail 3x before cooldown
  cooldown_time: 15  # 30 second cooldown = quick recovery
  client_ttl: 3600  # Cache HTTP clients for 1hr = reuse connections
  optional_pre_call_checks: ["prompt_caching"]  # ✅ Enables OpenAI prompt caching support

# ============================================================================
# IMAGE HANDLING + MULTI-LAYER CACHING (OPTIMAL SETUP):
# ============================================================================
# 
# ✅ Use IMAGE URLs instead of base64 for 50-100x better performance:
#
# CACHING BENEFITS (all work with image URLs):
#   1. LiteLLM Cache: Instant response if exact request seen before
#   2. OpenAI Prompt Cache: 50% cost reduction (images cached 5-10 min)
#   3. LiteLLM Image Cache: In-memory cache of downloaded images (if needed)
#
# PERFORMANCE COMPARISON (for 1000 requests with cache hits):
#   ❌ Base64: 2-5 GB uploaded, 500ms per request
#   ✅ URLs:   ~1 MB uploaded,  10ms per request (50-100x faster!)
#
# IMPLEMENTATION:
#   messages=[{
#       "role": "user",
#       "content": [
#           {"type": "text", "text": "Extract text"},
#           {"type": "image_url", "image_url": {
#               "url": "https://your-cdn.com/pdf-123/page-1.jpg"  # Stable URL
#           }}
#       ]
#   }]
#
# REQUIREMENTS for OpenAI Prompt Caching:
#   - Minimum 1024 tokens required
#   - LiteLLM uses DEFAULT_IMAGE_TOKEN_COUNT (default: 250) for caching checks
#   - YOUR IMAGES: ~2000 tokens each (130 DPI high-res)
#   
#   ⚠️  IMPORTANT: Set DEFAULT_IMAGE_TOKEN_COUNT=2000 in Kubernetes deployment
#       This matches your actual image token usage and enables prompt caching!
#       Without this, single images won't trigger caching (250 < 1024)
#   
#   To set: kubectl set env deployment/litellm DEFAULT_IMAGE_TOKEN_COUNT=2000 -n litellm
#   
#   - Use stable URLs (not signed/expiring URLs)
#   - Cache expires after 5-10 min of inactivity
#   - Works automatically with optional_pre_call_checks: ["prompt_caching"]
#
# NOTE: LiteLLM auto-converts URLs to base64 if provider doesn't support URLs
# ============================================================================